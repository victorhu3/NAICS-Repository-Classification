{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akwZJCUKnOET"
      },
      "outputs": [],
      "source": [
        "!pip install Github\n",
        "!pip install PyGithub\n",
        "!pip install github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JF_txQ-IY9d"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ6O-AuBIg1O"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nFSYgZWCIlk0"
      },
      "outputs": [],
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1PqMPI-nJmD"
      },
      "outputs": [],
      "source": [
        "#change main topic and num of repos\n",
        "main_topic = \"Other Services\"\n",
        "num_of_repos = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AzfNQf2IoZL"
      },
      "outputs": [],
      "source": [
        "#verify conda\n",
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zocy60aXItYt"
      },
      "outputs": [],
      "source": [
        "!conda install gh --channel conda-forge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NAeGhv8IxLM"
      },
      "outputs": [],
      "source": [
        "#verify github cli installation\n",
        "!gh --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kWyyT-nIgRa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import base64\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from requests.structures import CaseInsensitiveDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsClqTotGSMK"
      },
      "outputs": [],
      "source": [
        "data_path = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyDpvZxFI4Oy"
      },
      "outputs": [],
      "source": [
        "!gh auth login --with-token < \"g_token.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wga656b3PSe6"
      },
      "outputs": [],
      "source": [
        "def get_topic_list(main_topic):\n",
        "  naics = pd.read_csv(\"NAICS Topics.csv\")\n",
        "  lst_of_topics = naics[naics[\"Definition\"] == main_topic][\"Related Github Topics\"].tolist()\n",
        "  return lst_of_topics[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghtmEwVVH-5L"
      },
      "outputs": [],
      "source": [
        "g = Github(\"GITHUB_PAT_HERE\")\n",
        "\n",
        "#function to get the repo name and github user for each repository grouped by the github topics they were searched for using\n",
        "def get_name_repo_txt(repo_topic, base_folder):\n",
        "  import re\n",
        "\n",
        "  # Input text\n",
        "  text = repo_topic\n",
        "\n",
        "  # Define a regex pattern to match the full_name value\n",
        "  pattern = r'full_name=\"([^\"]+)\"'\n",
        "\n",
        "  # Use re.search to find the match\n",
        "  match = re.search(pattern, text)\n",
        "\n",
        "  # Check if a match is found\n",
        "  if match:\n",
        "      # Extract the full_name value from the match\n",
        "      full_name = match.group(1)\n",
        "      print(\"Full Name:\", full_name)\n",
        "  else:\n",
        "      print(\"No match found.\")\n",
        "\n",
        "  #search for owner:\n",
        "  text = full_name\n",
        "\n",
        "  #define pattern to match owner value:\n",
        "  pattern = r'(.+)/'\n",
        "\n",
        "  # Use re.search to find the match\n",
        "  match = re.search(pattern, text)\n",
        "\n",
        "  # Check if a match is found\n",
        "  if match:\n",
        "      # Extract the full_name value from the match\n",
        "      owner = match.group(1)\n",
        "      print(\"Owner:\", owner)\n",
        "  else:\n",
        "      print(\"No match found.\")\n",
        "\n",
        "  #search for repo:\n",
        "  text = full_name\n",
        "\n",
        "  #define pattern to match repo value:\n",
        "  pattern = r'/(.+)'\n",
        "\n",
        "  # Use re.search to find the match\n",
        "  match = re.search(pattern, text)\n",
        "\n",
        "  # Check if a match is found\n",
        "  if match:\n",
        "      # Extract the full_name value from the match\n",
        "      repo = match.group(1)\n",
        "      print(\"Repo:\", repo)\n",
        "  else:\n",
        "      print(\"No match found.\")\n",
        "  # Define a list of user and repo pairs\n",
        "  data = [f\"{owner}\", f\"{repo}\"]\n",
        "  print(data)\n",
        "  return data\n",
        "\n",
        "\n",
        "#places the file with all of the repos into the drive\n",
        "def put_txt_in_folder(topic_input_list, base_folder_path, num_of_repos, Main_Topic):\n",
        "  # Split the input string into a list of strings\n",
        "  topics = topic_input_list.split(\", \")\n",
        "  ORGANIZATION = 'your_github_org_name'\n",
        "  listofrepos = []\n",
        "  repotopics = []\n",
        "  per_topic = num_of_repos\n",
        "  for TOPIC in topics:\n",
        "    repos = g.search_repositories(query=f'topic:{TOPIC}')\n",
        "    iteration_counter = 0\n",
        "    while iteration_counter <= per_topic:\n",
        "      repo_string = str(repos[iteration_counter])\n",
        "      try:\n",
        "        listofrepos.append(repo_string)\n",
        "        repotopics.append(TOPIC)\n",
        "      except:\n",
        "        continue\n",
        "      iteration_counter += 1\n",
        "    print(len(listofrepos))\n",
        "\n",
        "  data = []\n",
        "  iteration_counter = 0\n",
        "  for (repo, topic) in zip(listofrepos, repotopics):\n",
        "    try:\n",
        "      print(repo)\n",
        "      new_entry = get_name_repo_txt(str(repo), base_folder_path)\n",
        "      new_entry.append(topic)\n",
        "      data.append(new_entry)\n",
        "      print(\"Success\")\n",
        "    except:\n",
        "      print(\"Failure\")\n",
        "    iteration_counter += 1\n",
        "    if iteration_counter > num_of_repos:\n",
        "      break\n",
        "  print(data)\n",
        "  data_path = \"./repo_by_topic\"\n",
        "  os.chdir(data_path)\n",
        "\n",
        "  if not os.path.exists(Main_Topic):\n",
        "      os.mkdir(Main_Topic)\n",
        "\n",
        "  os.chdir(Main_Topic)\n",
        "  new_file_name = f\"topics_{Main_Topic}.csv\"\n",
        "\n",
        "  # Create a subfolder name based on the README content (you can adjust this logic)\n",
        "  #subfolder_name = f\"{Main_Topic}\"  # Use the first 10 characters as the subfolder name\n",
        "\n",
        "  # Construct the subfolder path\n",
        "  #subfolder_path = os.path.join(base_folder_path, subfolder_name)\n",
        "\n",
        "  # Create the subfolder if it doesn't exist\n",
        "  #os.makedirs(subfolder_path, exist_ok=True)\n",
        "  # Define the new file name\n",
        "\n",
        "  # Write the new file content to a file inside the same subfolder\n",
        "  new_file_path = os.path.join(data_path, new_file_name)\n",
        "  # Open the file in append mode\n",
        "  with open(new_file_name, \"a\") as file:\n",
        "    # Iterate over the new_data and append each pair to the file in the desired format\n",
        "    file.write(f\"{Main_Topic}\\n\")\n",
        "    file.write(\"User, Repo, Topic\\n\")\n",
        "    for lst in data:\n",
        "        file.write(f\"{lst[0]}, {lst[1]}, {lst[2]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyVIrNF1I-r7"
      },
      "outputs": [],
      "source": [
        "#gets the REST API url for a certain repository\n",
        "def get_url(user, repo, path=\"\"):\n",
        "  if path != \"\":\n",
        "    return \"https://api.github.com/repos/\" + user + \"/\" + repo\n",
        "  else:\n",
        "    return \"https://api.github.com/repos/\" + user + \"/\" + repo + \"/\" + path\n",
        "\n",
        "def file_to_list_dict(filepath):\n",
        "  with open(filepath) as file:\n",
        "        content = file.read()\n",
        "  result = []\n",
        "  i = 0\n",
        "  while i < len(content):\n",
        "      if content[i] == '{':\n",
        "        start = i\n",
        "        cb1 = 1\n",
        "        cb2 = 0\n",
        "        while cb1 != cb2:\n",
        "            i+=1\n",
        "            if content[i] == \"{\":\n",
        "              cb1+=1\n",
        "            elif content[i] == '}':\n",
        "              cb2+=1\n",
        "        result.append(json.loads(content[start:i+1]))\n",
        "      i+=1\n",
        "  return result\n",
        "\n",
        "#gets the info in the category section of the content of the repository\n",
        "def get(user, repo, category, nested=False):\n",
        "    rdict = get_content(user, repo)\n",
        "    if category in rdict:\n",
        "      return rdict[category]\n",
        "    else:\n",
        "      return \"N/A\"\n",
        "\n",
        "#gets the readme for a certain repository given its name and the github user\n",
        "def get_readme(user, repo):\n",
        "  rdict = get_content(user, repo, path=\"contents/README.md\")\n",
        "  if 'content' in rdict:\n",
        "    decoded = base64.b64decode(rdict['content']).decode('utf-8')\n",
        "    return decoded\n",
        "  else:\n",
        "    return rdict\n",
        "\n",
        "#gets the content json for a certain repository given its name and the github user\n",
        "def get_content(user, repo, path=\"\"):\n",
        "  '''\n",
        "  Equivalent of:\n",
        "    !curl -L \\\n",
        "    -H 'Accept: application/vnd.github+json' \\\n",
        "    -H 'Authorization: Bearer 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' \\\n",
        "    -H 'X-Github-Api-Version: 2022-11-28' \\\n",
        "    https://api.github.com/repos/<user>/<repo>/contents/README.md > <user>-<repo>.json\n",
        "  '''\n",
        "  url = \"https://api.github.com/repos/\" + user + \"/\" + repo\n",
        "\n",
        "  if path != \"\":\n",
        "    url += \"/\" + path\n",
        "\n",
        "  headers = CaseInsensitiveDict()\n",
        "  headers[\"Accept\"] = \"application/vnd.github+json\"\n",
        "  headers[\"Authorization\"] = \"Bearer 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\"\n",
        "  headers[\"X-Github-Api-Version\"] = \"2022-11-28\"\\\n",
        "\n",
        "  resp = requests.get(url, headers=headers)\n",
        "  rdict = json.loads(resp.text)\n",
        "  return rdict\n",
        "\n",
        "#reads the repositories from a file\n",
        "def get_repos(filepath):\n",
        "  lst = []\n",
        "  f = open(filepath, 'r')\n",
        "  f.readline()\n",
        "  f.readline()\n",
        "  curr = f.readline().split(\",\")\n",
        "  while len(curr) > 1:\n",
        "    lst.append((curr[0].strip(), curr[1].strip(), curr[2][:-1].strip()))\n",
        "    curr = f.readline().split(\",\")\n",
        "  f.close()\n",
        "  return lst\n",
        "\n",
        "#gets all of the necessary data for a list of repositories and stores it in a dataframe. The newly created dataframe is returned.\n",
        "def get_data(topic, repos):\n",
        "  cols = {\"repo\": [], \"user\": [], \"organization\": [], \"url (HTML)\": [], \"url (API)\": [], \"description\": [], \"readme\": [], \"stargazer count\": [], \"watcher count\": [], \"subscriber count\": [], \"open issue count\": [], \"topic (search)\": [], \"topics\": [], \"NAICS Code\": []}\n",
        "  data = pd.DataFrame(cols)\n",
        "\n",
        "  reposlst = []\n",
        "\n",
        "  for r in repos:\n",
        "    row = []\n",
        "\n",
        "    rname = r[1]\n",
        "    ruser = r[0]\n",
        "    rtopic = r[2]\n",
        "\n",
        "    row.extend([rname, ruser])\n",
        "\n",
        "    url_content = get_content(ruser, rname)\n",
        "    reposlst.append(url_content)\n",
        "\n",
        "    organization = get(ruser, rname, \"organization\")\n",
        "    if organization != \"N/A\":\n",
        "      row.append(organization['login'])\n",
        "    else:\n",
        "      row.append(organization)\n",
        "    row.append(get(ruser, rname, \"html_url\"))\n",
        "    row.append(get_url(ruser, rname)[:-1])\n",
        "    row.append(get(ruser, rname, \"description\"))\n",
        "    row.append(get_readme(ruser, rname))\n",
        "    row.append(get(ruser, rname, \"stargazers_count\"))\n",
        "    row.append(get(ruser, rname, \"watchers_count\"))\n",
        "    row.append(get(ruser, rname, \"subscribers_count\"))\n",
        "    row.append(get(ruser, rname, \"open_issues_count\"))\n",
        "    row.append(rtopic)\n",
        "    row.append(get(ruser, rname, \"topics\"))\n",
        "    row.append(\"00000\")\n",
        "\n",
        "    data.loc[len(data.index)] = row\n",
        "  return data, reposlst\n",
        "\n",
        "#saves the data and list of repos as csv and txt files\n",
        "def save_data(topic, data, reposlst):\n",
        "  data_path = \"./repo_by_topic/\" + topic + \"/\"\n",
        "  os.chdir(data_path)\n",
        "\n",
        "  frepos = open(\"repos_\" + topic + \".txt\", \"w\")\n",
        "  frepos.write(str(reposlst))\n",
        "  frepos.close()\n",
        "\n",
        "  data.to_csv(\"data_\" + topic + \".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGUo8ZkyEYlB"
      },
      "outputs": [],
      "source": [
        "#updates the dataframe with new data (only use if more data needs to be added to a .csv file)\n",
        "def update_data(topic, new_data, new_repo):\n",
        "  data_path = \"./repo_by_topic/\" + topic + \"/\"\n",
        "  os.chdir(data_path)\n",
        "\n",
        "  frepos = open(\"repos_\" + topic + \".txt\", \"a\")\n",
        "  frepos.write(str(new_repo))\n",
        "  frepos.close()\n",
        "\n",
        "  curr = pd.DataFrame()\n",
        "  if not os.path.exists(data_path + \"data_\" + topic + \".csv\"):\n",
        "    cols = {\"repo\": [], \"user\": [], \"organization\": [], \"url (HTML)\": [], \"url (API)\": [], \"description\": [], \"readme\": [], \"stargazer count\": [], \"watcher count\": [], \"subscriber count\": [], \"open issue count\": [], \"topic (search)\": [], \"topics\": [], \"NAICS Code\": []}\n",
        "    curr = pd.DataFrame(cols)\n",
        "  else:\n",
        "    curr = pd.read_csv(\"data_\" + topic + \".csv\")\n",
        "\n",
        "  new_df = pd.concat([curr, new_data])\n",
        "  if \"Unnamed: 0\" in new_df.columns:\n",
        "    new_df = new_df.drop(columns=[\"Unnamed: 0\"])\n",
        "  new_df.to_csv(\"data_\" + topic + \".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e02r63jkjcs"
      },
      "outputs": [],
      "source": [
        "#gets the topic list for a certain code, so those can be searched on\n",
        "data_path = \".\"\n",
        "\n",
        "main_topic_list = get_topic_list(main_topic)\n",
        "put_txt_in_folder(main_topic_list, data_path, num_of_repos, main_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKndlO-fJOgj"
      },
      "outputs": [],
      "source": [
        "#saves data for a certain code in .csv and .txt files\n",
        "data_path = \"./repo_by_topic/\" + main_topic + \"/\"\n",
        "os.chdir(data_path)\n",
        "file = \"topics_\" + main_topic + \".csv\"\n",
        "repos = get_repos(file)\n",
        "data, reposlst = get_data(main_topic, repos)\n",
        "save_data(main_topic, data, reposlst)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
